# AI Chat Analysis - Trained Model Documentation

## Overview
The AI Chat Analysis service has been enhanced with machine learning capabilities trained on **20,009 synthetic chat messages**. The service automatically learns patterns from real project communication data and provides intelligent weekly summaries for mentors.

## üéì Training Process

### What Gets Trained
The AI service trains on your synthetic chat data to recognize:

1. **Task Completion Patterns**
   - Identifies when team members complete tasks
   - Keywords: "completed", "finished", "done", "merged", "deployed", "ready"
   - Learns 50+ common completion phrases

2. **Blocker Patterns**
   - Detects when team members report obstacles
   - Keywords: "blocked", "stuck", "issue", "problem", "error", "crash", "fail"
   - Tracks blocker frequency and severity

3. **Progress Update Patterns**
   - Recognizes status updates and work-in-progress messages
   - Keywords: "working on", "started", "in progress", "update", "pushed"
   - Monitors team velocity

4. **Collaboration Patterns**
   - Identifies helpful interactions and peer support
   - Keywords: "help", "review", "check", "thanks", "lgtm", "looks good"
   - Calculates collaboration score (0-100)

### Training Data
- **Source**: `synthetic_chats.csv` (20,009 messages)
- **Users**: 20 different team members
- **Projects**: 5 different projects (Apollo, Beacon, Catalyst, Drift, Eclipse)
- **Roles**: member, team_lead, mentor, observer
- **Time Span**: 90 days of simulated activity

### Automatic Training
The service **automatically trains on startup** by:
1. Loading all messages from `synthetic_chats.csv`
2. Analyzing patterns using spaCy NLP
3. Building keyword frequency maps
4. Extracting common technical terms
5. Calculating baseline metrics

## üìä Enhanced Weekly Report Features

The trained model provides comprehensive weekly summaries with:

### 1. **Core Metrics**
- Total messages exchanged
- Number of active participants
- Overall team sentiment (-1 to +1 scale)
- Project momentum (Strong/Moderate/Weak)

### 2. **Task Tracking** (NEW)
- ‚úÖ **Tasks Completed**: Count of finished work items
- üöß **Blockers Reported**: Issues and obstacles identified
- üìù **Progress Updates**: Status updates shared
- ü§ù **Collaboration Score**: Team helping behavior (0-100)

### 3. **Top Contributors Analysis**
For each contributor, the report shows:
- **Technical Contribution Score** (0-100)
  - 40% weight on technical content
  - 25% weight on problem solving
  - 15% weight on helping others
  - 20% weight on task completion
- **Contribution Quality**: High/Medium/Low classification
- **Code-related messages**: Count of technical discussions
- **Problem-solving count**: How often they solve issues
- **Help given count**: Peer support instances
- **Technical keywords used**: Specific technologies mentioned

### 4. **Key Discussions**
- Extracts important conversations about:
  - Task completions
  - Deployments
  - Issues and problems
  - Solutions implemented
- Presents top 8 discussions with username context

### 5. **Technical Topics**
- Identifies main focus areas using NLP
- Tracks topic frequency across messages
- Filters using trained keyword patterns
- Shows top 10 topics being discussed

### 6. **AI-Powered Recommendations**
The trained model generates smart recommendations about:

#### Task & Progress
- Alerts if no tasks completed
- Warns about low completion rates
- Highlights strong performance

#### Blockers & Issues
- Escalates if many blockers reported
- Recommends team syncs for resolution
- Tracks blocker resolution progress

#### Collaboration
- Encourages peer support if low
- Celebrates strong collaboration
- Identifies collaboration champions

#### Team Activity
- Compares communication volume to baseline
- Suggests more standups if too quiet
- Recognizes high engagement

#### Sentiment Analysis
- Warns about negative sentiment trends
- Acknowledges positive team morale
- Recommends check-ins when needed

#### Inactive Members
- Identifies users with minimal contribution
- Suggests follow-ups on progress
- Tracks who needs support

## üöÄ API Endpoints

### 1. Health Check
```http
GET /
```
Response:
```json
{
  "status": "ok",
  "trained": true,
  "training_messages": 20009
}
```

### 2. Training Statistics
```http
GET /training_stats
```
Response:
```json
{
  "total_training_messages": 20009,
  "avg_message_length": 5.9,
  "patterns_learned": {
    "task_completion": 50,
    "blockers": 50,
    "progress_updates": 50,
    "collaboration": 50
  },
  "top_keywords": ["update", "review", "deploy", ...],
  "active_users_in_training": 20
}
```

### 3. Manual Retrain
```http
POST /retrain
```
Forces the model to retrain on current CSV data.

### 4. Weekly Mentor Report (Enhanced)
```http
POST /weekly_report
```
Request body:
```json
{
  "messages": [
    {
      "username": "alice",
      "text": "Completed the authentication module",
      "timestamp": "2025-11-17T10:00:00Z"
    }
  ]
}
```

Response includes:
```json
{
  "report_period": "Week of November 17, 2025",
  "total_messages": 100,
  "total_participants": 15,
  "overall_sentiment": 0.45,
  "project_momentum": "Strong",
  "tasks_completed": 12,
  "blockers_reported": 3,
  "progress_updates": 25,
  "collaboration_score": 75.5,
  "top_contributors": [...],
  "key_discussions": [...],
  "technical_topics": [...],
  "recommendations": [...],
  "activity_summary": "..."
}
```

### 5. User Participation Ranking
```http
POST /analyze_users
```
Ranks users by participation score (unchanged from original).

### 6. Basic Analysis
```http
POST /analyze
```
Returns sentiment and keywords (unchanged from original).

## üß™ Testing

Run the test suite to verify training:
```bash
cd ai-chat-analysis
python test_trained_model.py
```

This will:
1. Check training status
2. Display training statistics
3. Generate a sample weekly report
4. Rank user participation
5. Verify all patterns are loaded

## üìà How Mentors Use This

### Weekly Review Process
1. **Automated Collection**: Your app sends all week's chat messages to `/weekly_report`
2. **AI Analysis**: The trained model analyzes patterns, sentiment, contributions
3. **Report Generation**: Comprehensive summary generated in seconds
4. **Mentor Review**: Mentor sees:
   - Who did actual work vs just chatting
   - What tasks were completed
   - Where blockers exist
   - Team morale status
   - Specific recommendations for improvement

### Key Insights for Mentors
- **Technical Contribution Scores** show who's coding vs discussing
- **Collaboration Scores** identify team players
- **Blocker Tracking** highlights where team needs help
- **Sentiment Analysis** reveals team morale issues early
- **Activity Patterns** show engagement trends

## üîß Customization

### Adding More Training Data
To retrain with new data:
1. Add more rows to `synthetic_chats.csv`
2. Restart the service (auto-trains on startup)
3. Or call `POST /retrain` endpoint

### Adjusting Pattern Recognition
Edit in `main.py`:
- `classify_message_type()` - Add new technical keywords
- `enhanced_message_classification()` - Modify pattern weights
- `analyze_user_contributions()` - Adjust scoring formula

### Tuning Recommendations
Edit the recommendation logic in `generate_weekly_mentor_report()`:
- Threshold values (e.g., tasks_completed thresholds)
- Sentiment boundaries
- Collaboration score criteria

## üìä Performance

- **Training time**: ~3-5 seconds for 20k messages
- **Analysis time**: ~2-3 seconds for 100 messages
- **Memory usage**: ~150MB (includes spaCy model)
- **Accuracy**: Based on pattern matching + NLP + VADER sentiment

## üéØ Benefits Over Basic Analysis

### Before (Basic)
- Simple keyword counting
- Basic sentiment score
- No context understanding
- Generic recommendations

### After (Trained)
- ‚úÖ Pattern recognition from real data
- ‚úÖ Context-aware analysis
- ‚úÖ Task completion tracking
- ‚úÖ Blocker identification
- ‚úÖ Collaboration scoring
- ‚úÖ Smart, specific recommendations
- ‚úÖ Baseline comparisons
- ‚úÖ Quality-based contributor ranking

## üöÄ Next Steps

1. **Production Use**: Connect your Express backend to use `/weekly_report`
2. **Expand Training**: Add real chat data to CSV for better patterns
3. **Fine-tune**: Adjust weights and thresholds based on your team
4. **Monitoring**: Use `/training_stats` to track model health
5. **Feedback Loop**: Retrain periodically as chat patterns evolve

## üìù Example Use Case

**Scenario**: Team of 5 working on a project for 1 week

**Input**: 150 chat messages from the week

**Output**: 
- "Team completed 8 tasks this week (strong momentum)"
- "Alice is top contributor with 85/100 technical score"
- "3 blockers reported - recommend team sync"
- "Collaboration score 72/100 - good peer support"
- "Sentiment positive (0.42) - team morale high"
- Specific recommendations for each team member

**Mentor Action**: 
- Review Bob's low activity (0 technical messages)
- Schedule blocker resolution meeting
- Acknowledge Alice's strong contribution
- Continue current practices for other team members
